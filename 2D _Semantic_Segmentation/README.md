# Semantic Segmentation for ADAS: U-Net vs DeepLabV3+

![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c?logo=pytorch&logoColor=white)
![Platform](https://img.shields.io/badge/Platform-HPC%20%7C%20SLURM-green)
![Dataset](https://img.shields.io/badge/Dataset-Cityscapes-orange)
![Classes](https://img.shields.io/badge/Classes-19-blueviolet)
![License](https://img.shields.io/badge/License-MIT-yellow)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)
![Course](https://img.shields.io/badge/Course-ADAS-informational)
![University](https://img.shields.io/badge/University-UniPR-darkblue)

<p align="center">
  <a href="#-project-overview"> <b>English Version</b></a> &nbsp;|&nbsp;
  <a href="#-descrizione-del-progetto"> <b>Versione Italiana</b></a>
</p>

---

<a name="-project-overview"></a>
## Project Overview

This repository contains the implementation and experimental results for the **2D Semantic Segmentation** project.

The project performs a systematic comparison between two major deep learning architectures â€” **U-Net** (implemented from scratch) and **DeepLabV3+** (with multiple training strategies) â€” applied to the [Cityscapes Dataset](https://www.cityscapes-dataset.com/) for pixel-level urban scene understanding.

---

### Key Goals

1. **Architecture Comparison** â€” Evaluate the trade-off between U-Net (lightweight, ~31M params) and DeepLabV3+ (state-of-the-art, ~63M params) on the same benchmark.
2. **Training Strategy Study** â€” Investigate three strategies: training from scratch, Coarse-to-Fine transfer, and ImageNet pretrained fine-tuning.
3. **HPC Pipeline** â€” Full SLURM-based training pipeline with automatic job dependency management.
4. **Rigorous Evaluation** â€” Val mIoU monitored every epoch; best model saved on validation performance, not final epoch.

---

### Architectures

#### U-Net (from scratch)
A fully custom encoder-decoder network with skip connections, implemented from scratch in PyTorch.
- **Encoder**: 5 levels of DoubleConv (Convâ†’BNâ†’ReLUÃ—2) + MaxPool 2Ã—2; channels: 64â†’128â†’256â†’512â†’1024
- **Decoder**: ConvTranspose2d upsampling + concatenation of skip connections
- **Output**: 1Ã—1 Conv â†’ 19 classes
- ~**31M parameters** | ~124 MB

#### DeepLabV3+ (ResNet50 backbone)
State-of-the-art segmentation model using Atrous Spatial Pyramid Pooling (ASPP).
- **Backbone**: ResNet50 â€” deep feature extractor with residual connections
- **ASPP**: Parallel atrous convolutions (rates 6, 12, 18) + Global Average Pooling â€” captures multi-scale context
- **Decoder**: Lightweight upsampling fusing high- and low-level features
- ~**63M parameters** | ~159 MB

---

### Training Strategies

| Strategy | Dataset | Starting Point | Epochs | Notes |
|---|---|---|---|---|
| **Scratch** | gtFine | Random weights | 50 | Baseline for both models |
| **Coarse â†’ Fine** | gtCoarse â†’ gtFine | Random â†’ internal transfer | 50 + 30 | Official Cityscapes benchmark approach |
| **Pretrained** | gtFine | **ImageNet backbone** | 40 | True fine-tuning; classifier head replaced for 19 classes |

**Common training details:**
- **Augmentation**: Random horizontal flip (50%), random scale+crop (75â€“125%), color jitter
- **Loss**: CrossEntropyLoss with `ignore_index=255`; ENet class weights for coarse phase
- **Optimizer**: Adam | **Scheduler**: ReduceLROnPlateau (monitors val mIoU)
- **Gradient Accumulation** (U-Net): Ã—8 steps â†’ effective batch size = 16
- **Best model**: saved at maximum val mIoU, evaluated every epoch on 100 val images

---

### ğŸ“Š Results

#### Quantitative â€” mIoU & Pixel Accuracy

| Model | mIoU | Pixel Accuracy |
|---|---|---|
| U-Net â€” Scratch | 57.44% | 93.05% |
| U-Net â€” Coarseâ†’Fine | 57.70% | 93.06% |
| DeepLabV3+ â€” Coarse | 63.26% | 92.11% |
| DeepLabV3+ â€” Coarseâ†’Fine | 70.34% | 94.83% |
| DeepLabV3+ â€” Pretrained | 73.00% | 95.02% |
| **DeepLabV3+ â€” Scratch** | **74.20%** | **95.33%** |

> Results computed on the Cityscapes validation set (500 images, 19 classes, `ignore_index=255`).
> Evaluated with `evaluate_metrics.py` using the best checkpoint (max val mIoU) for each model.

---

### ğŸ“‚ Repository Structure

```
ğŸ“¦ HPC_Submission/
â”‚
â”œâ”€â”€ ğŸ“„ config.py                        # Paths, hyperparameters, device detection
â”œâ”€â”€ ğŸ“„ requirements.txt                 # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“ models/
â”‚   â””â”€â”€ unet_model.py                   # U-Net custom implementation
â”‚
â”œâ”€â”€ ğŸ“ utils/
â”‚   â”œâ”€â”€ dataset.py                      # Cityscapes DataLoader (with augmentation)
â”‚   â”œâ”€â”€ metrics.py                      # mIoU + validate_model()
â”‚   â””â”€â”€ class_weights.py               # ENet class weights for imbalance handling
â”‚
â”œâ”€â”€ ğŸ“ scripts_training_final/
â”‚   â”œâ”€â”€ train_coarse.py                 # U-Net â€” Coarse phase
â”‚   â”œâ”€â”€ train_fine.py                   # U-Net â€” Fine-tuning
â”‚   â”œâ”€â”€ train_unet_scratch_base.py      # U-Net â€” Scratch
â”‚   â”œâ”€â”€ train_deeplab_coarse.py         # DeepLab â€” Coarse phase
â”‚   â”œâ”€â”€ train_deeplab_fine.py           # DeepLab â€” Fine-tuning
â”‚   â”œâ”€â”€ train_deeplab_scratch.py        # DeepLab â€” Scratch
â”‚   â”œâ”€â”€ train_deeplab_pretrained.py     # DeepLab â€” ImageNet pretrained
â”‚   â”œâ”€â”€ submit_all.sh                   # Submit all jobs with dependencies
â”‚   â”œâ”€â”€ run_coarse.sh                   # SLURM â€” U-Net Coarse
â”‚   â”œâ”€â”€ run_fine.sh                     # SLURM â€” U-Net Fine
â”‚   â”œâ”€â”€ run_unet_scratch.sh             # SLURM â€” U-Net Scratch
â”‚   â”œâ”€â”€ run_deeplab_coarse.sh           # SLURM â€” DeepLab Coarse
â”‚   â”œâ”€â”€ run_deeplab_fine.sh             # SLURM â€” DeepLab Fine
â”‚   â”œâ”€â”€ run_deeplab_scratch.sh          # SLURM â€” DeepLab Scratch
â”‚   â””â”€â”€ run_deeplab_pretrained.sh       # SLURM â€” DeepLab Pretrained
â”‚
â”œâ”€â”€ ğŸ“ checkpoints/                     # Trained model weights (.pth) â€” not tracked by git
â”œâ”€â”€ ğŸ“ data/                            # Cityscapes dataset â€” not tracked by git
â”œâ”€â”€ ğŸ“ results_comparison/              # Visual segmentation outputs
â”œâ”€â”€ ğŸ“ thesis_plots/                    # Training curves and logs
â”œâ”€â”€ ğŸ“ presentation/                    # Slide content 
â”‚
â”œâ”€â”€ ğŸ“„ evaluate_metrics.py              # Full benchmark on val set
â”œâ”€â”€ ğŸ“„ predict_all.py                   # Inference + visualization
â”œâ”€â”€ ğŸ“„ model_complexity.py              # Parameter count
â””â”€â”€ ğŸ“„ plot_real_loss.py                # Training curve plots
```

---

### Usage

#### 1. Local Inference (Requires `.pth` checkpoints)

```bash
# Generate visual comparisons for all trained models
python3 predict_all.py

# Run full benchmark (mIoU + Pixel Accuracy on 500 val images)
python3 evaluate_metrics.py
```

#### 2. HPC Training â€” All jobs at once (recommended)

```bash
# Upload code to HPC (excludes checkpoints and data)
rsync -avz --progress \
  --exclude='checkpoints/' --exclude='data/' \
  --exclude='__pycache__/' --exclude='*.pyc' \
  ./ aurora.felisari@login.hpc.unipr.it:~/project/

# Connect and submit
ssh aurora.felisari@login.hpc.unipr.it
cd ~/project/scripts_training_final
sed -i 's/\r//' *.sh
bash submit_all.sh
```

`submit_all.sh` automatically handles dependencies:
- 5 independent jobs start immediately in parallel
- Fine-tuning jobs start automatically once their Coarse phase completes

#### 3. HPC Training â€” Single job

```bash
cd ~/project/scripts_training_final
sbatch run_deeplab_pretrained.sh
```

#### 4. Monitor jobs

```bash
squeue -u $USER
tail -f log_unet_scratch_<JOBID>.txt
```

---

### âš™ï¸ Setup

```bash
# Clone the repository
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>

# Install dependencies
pip install -r requirements.txt

# Download Cityscapes dataset
# Place it under: data/cityscapes/leftImg8bit/ and data/cityscapes/gtFine/
```

---

### Key Implementation Details

- **Class imbalance** handled via ENet-derived weights passed to `CrossEntropyLoss`
- **Augmentation** applied synchronously on image and mask using `torchvision.transforms.functional`
- **Pretrained fine-tuning**: classifier head replaced (`Conv2d(256â†’19)`), differential learning rates for backbone vs classifier
- **Paths**: `config.py` uses `os.path.abspath(__file__)` â€” works correctly regardless of launch directory
- **Memory**: `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` set in all training scripts

---

### Author

**Aurora Felisari**
UniversitÃ  di Parma â€” MS in Computer Science

---

---

<a name="-descrizione-del-progetto"></a>
## Descrizione del Progetto

Questa repository contiene l'implementazione e i risultati sperimentali del progetto di **Segmentazione Semantica 2D**.

Il progetto confronta sistematicamente due architetture di deep learning â€” **U-Net** (implementata da zero) e **DeepLabV3+** (con diverse strategie di training) â€” applicate al [dataset Cityscapes](https://www.cityscapes-dataset.com/) per la classificazione a livello di pixel di scene urbane.

---

### Obiettivi Principali

1. **Confronto Architetturale** â€” Analisi del compromesso tra U-Net (leggera, ~31M parametri) e DeepLabV3+ (stato dell'arte, ~63M parametri) sullo stesso benchmark.
2. **Studio delle Strategie di Training** â€” Confronto tra training da zero, transfer Coarseâ†’Fine e fine-tuning con backbone ImageNet pretrained.
3. **Pipeline HPC** â€” Training completo su cluster SLURM con gestione automatica delle dipendenze tra job.
4. **Valutazione Rigorosa** â€” Val mIoU monitorato ogni epoca; miglior modello salvato sulla performance di validazione, non sull'ultima epoca.

---

### Architetture

#### U-Net (da zero)
Rete encoder-decoder con skip connections, implementata completamente da zero in PyTorch.
- **Encoder**: 5 livelli DoubleConv (Convâ†’BNâ†’ReLUÃ—2) + MaxPool 2Ã—2; canali: 64â†’128â†’256â†’512â†’1024
- **Decoder**: Upsampling con ConvTranspose2d + concatenazione delle skip connections
- **Output**: Conv 1Ã—1 â†’ 19 classi
- ~**31M parametri** | ~124 MB

#### DeepLabV3+ (backbone ResNet50)
Modello all'avanguardia per la segmentazione che utilizza l'Atrous Spatial Pyramid Pooling (ASPP).
- **Backbone**: ResNet50 â€” feature extractor profondo con connessioni residuali
- **ASPP**: Convoluzioni dilatate parallele (tassi 6, 12, 18) + Global Average Pooling â€” cattura contesto multi-scala
- **Decoder**: Upsampling leggero che fonde feature di alto e basso livello
- ~**63M parametri** | ~159 MB

---

### Strategie di Training

| Strategia | Dataset | Punto di partenza | Epoche | Note |
|---|---|---|---|---|
| **Scratch** | gtFine | Pesi random | 50 | Baseline per entrambi i modelli |
| **Coarse â†’ Fine** | gtCoarse â†’ gtFine | Random â†’ transfer interno | 50 + 30 | Approccio ufficiale del paper Cityscapes |
| **Pretrained** | gtFine | **Backbone ImageNet** | 40 | Vero fine-tuning; head sostituita per 19 classi |

**Dettagli comuni:**
- **Augmentation**: Random horizontal flip (50%), random scale+crop (75â€“125%), color jitter
- **Loss**: CrossEntropyLoss con `ignore_index=255`; class weights ENet per la fase coarse
- **Optimizer**: Adam | **Scheduler**: ReduceLROnPlateau (monitora val mIoU)
- **Gradient Accumulation** (U-Net): Ã—8 step â†’ batch effettivo = 16
- **Best model**: salvato al massimo val mIoU, calcolato ogni epoca su 100 immagini di validation

---

### Risultati

#### Quantitativi â€” mIoU e Pixel Accuracy

| Modello | mIoU | Pixel Accuracy |
|---|---|---|
| U-Net â€” Scratch | 57.44% | 93.05% |
| U-Net â€” Coarseâ†’Fine | 57.70% | 93.06% |
| DeepLabV3+ â€” Coarse | 63.26% | 92.11% |
| DeepLabV3+ â€” Coarseâ†’Fine | 70.34% | 94.83% |
| DeepLabV3+ â€” Pretrained | 73.00% | 95.02% |
| **DeepLabV3+ â€” Scratch** | **74.20%** | **95.33%** |

> Risultati calcolati sul validation set di Cityscapes (500 immagini, 19 classi, `ignore_index=255`).
> Valutato con `evaluate_metrics.py` usando il miglior checkpoint (max val mIoU) per ogni modello.

---

### Utilizzo

#### 1. Inferenza in locale (richiede i checkpoint `.pth`)

```bash
# Genera le visualizzazioni comparative per tutti i modelli
python3 predict_all.py

# Esegui il benchmark completo (mIoU + Pixel Accuracy su 500 immagini)
python3 evaluate_metrics.py
```

#### 2. Training su HPC â€” Tutti i job in una volta (raccomandato)

```bash
# Carica il codice sull'HPC (escludi checkpoint e dati)
rsync -avz --progress \
  --exclude='checkpoints/' --exclude='data/' \
  --exclude='__pycache__/' --exclude='*.pyc' \
  ./ aurora.felisari@login.hpc.unipr.it:~/project/

# Connettiti e lancia
ssh aurora.felisari@login.hpc.unipr.it
cd ~/project/scripts_training_final
sed -i 's/\r//' *.sh   
bash submit_all.sh
```

`submit_all.sh` gestisce automaticamente le dipendenze:
- 5 job indipendenti partono subito in parallelo
- I fine-tuning partono automaticamente al completamento della fase coarse

#### 3. Training su HPC â€” Singolo job

```bash
cd ~/project/scripts_training_final
sbatch run_deeplab_pretrained.sh
```

#### 4. Monitoraggio

```bash
squeue -u $USER
tail -f log_unet_scratch_<JOBID>.txt
```

---

### âš™ï¸ Setup

```bash
# Clona la repository
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>

# Installa le dipendenze
pip install -r requirements.txt

# Scarica il dataset Cityscapes
# Posizionalo in: data/cityscapes/leftImg8bit/ e data/cityscapes/gtFine/
```

---

### Dettagli Implementativi

- **Sbilanciamento delle classi** gestito tramite class weights ENet nella `CrossEntropyLoss`
- **Augmentation** applicata sincronizzata su immagine e maschera con `torchvision.transforms.functional`
- **Fine-tuning pretrained**: head classificatore sostituita (`Conv2d(256â†’19)`), learning rate differenziato backbone vs classificatore
- **Path assoluti**: `config.py` usa `os.path.abspath(__file__)` â€” funziona indipendentemente dalla directory di lancio
- **Memoria GPU**: `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` impostato in tutti gli script

---

### Autore

**Aurora Felisari**
UniversitÃ  di Parma â€” Studentessa Magistrale in Scienze Informatiche
